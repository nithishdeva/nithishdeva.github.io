---
layout: post
title: Differentiable Computations
date: 2018-09-13 09:53 +05:30
comments: true
---

<p class="font-weight-light p-2">
This article is about a technique to make any series of computations differentiable. We will demonstrate the technique to derive back propagation algorithm in a slightly weird way.
</p> 
<h1 class="text-left">A generalised view of Computations</h1>
Auto gradient is a nice feature found in many computational frameworks. Specify the computation in forward direction and the framework computes backward gradients. Lets talk about the generic method to do this.</p><p>Lets say we have to compute the result of `something'. It may be a nasty heat equation or some logic driven steps to get from input to output. Abstracting the steps involed gives us a sequence of equations <script type="math/tex; mode=display"> 
\begin{aligned}
  z_i = f_i(z_{a(i)})
\end{aligned}
</script> 
<span style="display:None">z_i = f_i(z_{a(i)})</span> The <script type="math/tex"> z </script>'s are intermediate variables of the computation steps or they may be parameters. The selections <script type="math/tex"> z_{a(i)} </script> are inputs to <script type="math/tex"> f_i </script>.</p><p>  <center>
<i>What does gradient of this sequence of computation mean?</i>
</center></p><p>If  is the final step of the computation, then computing gradients of the sequence  means i</p><p><script type="math/tex"> \frac{\partial z_n}{\partial z_i} </script> are the gradients if <script type="math/tex"> z_n=f_n(z_{a(n)}) </script> is the final step. Computing all those gradients gives us how parameters change w.r.to the output.</p><p>
<h2 class="text-left">Handling Branches and loops</h2>
For any general computation to be included, we need to talk about branches and loops. How are these handled in our model?</p><p>Conditional branches can be represented by indicator functions. See <a href="https://en.wikipedia.org/wiki/Indicator_function#Derivatives_of_the_indicator_function">this entry</a> for details on computing derivative of indicator functions. </p><p>Loops could be unrolled in to a sequence of functions. All of them would simply share a same parameters, but inputs will be output of the function representing previous iteration. For example <pre>
<code class="python">
begin loop 1:3
  x = x + c
end
</code>
</pre></p><p>can be unrolled as</p><p><script type="math/tex; mode=display"> 
\begin{aligned}
  x_1 &= x + c
\\x_2 &= x_1 + c
\\x_3 &= x_2 + c
\end{aligned}
</script> 
<span style="display:None">x_1 &= x + c
\\x_2 &= x_1 + c
\\x_3 &= x_2 + c</span></p><p>This won't work for infinite loops because the unrolling will never end. Infinite loops has no business in real world computation. If a loop cannot be unrolled even after applying the ``reality of the universe'', we are not talking about a computational system . It might be an event loop or a queue. Neither needs gradients!</p><p>
<h1 class="text-left">Forward computation as a Constrained optimisation problem</h1>
Without loss of generality, we can say that all this hoopla of computing gradient is to minimise the final value. Even if this is not the case, like for example, if maximising the final result was the intent, then append a negating function at the end of the sequence. There are many other techniques out there to convert different problems to a minimization problem. </p><p>Now that we have <i>that</i> out of the way, lets look at the following problem. <script type="math/tex; mode=display"> 
\begin{aligned}
  &\min{z_n}
  \\
  s.t~z_i &= f_i(z_{a(i)})
\end{aligned}
</script> 
<span style="display:None">&\min{z_n}
  \\
  s.t~z_i &= f_i(z_{a(i)})</span></p><p>The formulation if a little bit weird. All it is saying is, minimise <script type="math/tex"> z_n </script> such that, outputs of computations (<script type="math/tex"> f_i </script>) are inputs to some other computation (all <script type="math/tex"> f </script>'s which has <script type="math/tex"> z_i </script> as input). Constraints are maintaining integrity of the sequence. So we managed to represent same thing is two ways, each saying the same thing. Great!</p><p>
<h1 class="text-left">How do you solve a constrained optimisation problem?</h1>
Using  the method of <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>.  It basically says that once we define Lagrange's function <script type="math/tex; mode=display"> 
\begin{aligned}
  L(z,\lambda) = z_n - \sum_i\lambda_i(z_i - f_i(z_{a(i)}))
\end{aligned}
</script> 
<span style="display:None">L(z,\lambda) = z_n - \sum_i\lambda_i(z_i - f_i(z_{a(i)}))</span></p><p>These <script type="math/tex"> L </script>'s gradient w.r.to its parameters vanishes at optimum points of original function as well. So we get <script type="math/tex; mode=display"> 
\begin{aligned}
  \nabla_{\lambda_i}=0 &\implies z_i = f_i(z_{a(i)})
  \\
  \nabla_{z_n}=0        &\implies \lambda_n = 1
  \\ 
  \nabla_{z_i}=0        &\implies \lambda_i = \sum_{k\in b(i)}\lambda_k \frac{\partial f_k}{\partial z_i}
\end{aligned}
</script> 
<span style="display:None">\nabla_{\lambda_i}=0 &\implies z_i = f_i(z_{a(i)})
  \\
  \nabla_{z_n}=0        &\implies \lambda_n = 1
  \\ 
  \nabla_{z_i}=0        &\implies \lambda_i = \sum_{k\in b(i)}\lambda_k \frac{\partial f_k}{\partial z_i}</span></p><p>Final expression of  <script type="math/tex"> \lambda_i </script>'s will give <script type="math/tex"> \frac{\partial z_n}{\partial z_i} </script> and hence all the gradients of our original computation. <script type="math/tex"> b(\cdot) </script> is like inverse of <script type="math/tex"> a(\cdot) </script>. <script type="math/tex"> a(i) </script> gives which <script type="math/tex"> z </script>'s are arguments of <script type="math/tex"> f_i </script> while <script type="math/tex"> b(i) </script> simply gives which <script type="math/tex"> f </script> has <script type="math/tex"> z_i </script> as an argument. <script type="math/tex"> b=a^{-1} </script> ??. Anyway, these equations fits nicely as a linear system</p><p><script type="math/tex; mode=display"> 
\begin{aligned}
  A\lambda = 
\begin{bmatrix}
0\\
\vdots\\
0\\
-1
\end{bmatrix}
\quad
; A_{k,i} = 
\begin{cases}
   \frac{\partial f_k}{\partial z_i} & k\in b(i)
\\ -1 & k=i
\\ 0 & otherwise
\end{cases}
\end{aligned}
</script> 
<span style="display:None">A\lambda = 
\begin{bmatrix}
0\\
\vdots\\
0\\
-1
\end{bmatrix}
\quad
; A_{k,i} = 
\begin{cases}
   \frac{\partial f_k}{\partial z_i} & k\in b(i)
\\ -1 & k=i
\\ 0 & otherwise
\end{cases}</span></p><p><script type="math/tex"> A </script> is an upper triangular matrix with 1's on the diagonal. Otherwise, we are looking at sequence of computation which needs result of a future. That is too complicated for now(example of explicit systems).</p><p>This linear system of equations opens up myriad of possibilities of computing gradients faster. The simplest of which is back substitution since <script type="math/tex"> A </script> is triangular. If the computation we are dealing with is a forward pass of a neural network, what we get out of the back substitution is ``backprop" algorithm!!</p><p>
<h1 class="text-left">Deriving backprop, in a weird way</h1>
Lets look at a very simple Neural network <script type="math/tex; mode=display"> 
\begin{aligned}
  a_1 &= \sigma(x w_1)
\\
a_2 &= \operatorname{sofmax}(a_1 w_2)
\\
l &= \operatorname{loss}(a_2,y)
\end{aligned}
</script> 
<span style="display:None">a_1 &= \sigma(x w_1)
\\
a_2 &= \operatorname{sofmax}(a_1 w_2)
\\
l &= \operatorname{loss}(a_2,y)</span> If we simplify (ahem!) it up according to our problem, we get <script type="math/tex; mode=display"> 
\begin{aligned}
  z_1&=x,~ z_2=y, z_3=w_1, z_4=w_2
\\z_5 &= z_1z_3
\\z_6 &= \sigma(z_5)
\\z_7 &= z_6z_4
\\z_8 &= \operatorname{softmax}(z_7)
\\z_9 &= \operatorname{loss}(z_8,z_2)
\end{aligned}
</script> 
<span style="display:None">z_1&=x,~ z_2=y, z_3=w_1, z_4=w_2
\\z_5 &= z_1z_3
\\z_6 &= \sigma(z_5)
\\z_7 &= z_6z_4
\\z_8 &= \operatorname{softmax}(z_7)
\\z_9 &= \operatorname{loss}(z_8,z_2)</span> This gives us the linear system <script type="math/tex; mode=display"> 
\begin{aligned}
  \begin{bmatrix}
\\-1 &   &   &   & \frac{\partial f_{5}}{\partial z_{1}} &   &   &   &   
\\   &-1 &   &   &   &   &   &   & \frac{\partial f_{9}}{\partial z_{2}} 
\\   &   &-1 &   & \frac{\partial f_{5}}{\partial z_{3}} &   &   &   &   
\\   &   &   &-1 &   &   & \frac{\partial f_{7}}{\partial z_{4}} &   &   
\\   &   &   &   &-1 & \frac{\partial f_{6}}{\partial z_{5}} &   &   &   
\\   &   &   &   &   &-1 & \frac{\partial f_{7}}{\partial z_{6}} &   &   
\\   &   &   &   &   &   &-1 & \frac{\partial f_{8}}{\partial z_{7}} &   
\\   &   &   &   &   &   &   &-1 & \frac{\partial f_{9}}{\partial z_{8}} 
\\   &   &   &   &   &   &   &   & -1 & 
\end{bmatrix}
\begin{bmatrix}
\lambda_{1}\\
\lambda_{2}\\
\lambda_{3}\\
\lambda_{4}\\
\lambda_{5}\\
\lambda_{6}\\
\lambda_{7}\\
\lambda_{8}\\
\lambda_{9}\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
0\\
0\\
0\\
0\\
0\\
0\\
-1
\end{bmatrix}
\end{aligned}
</script> 
<span style="display:None">\begin{bmatrix}
\\-1 &   &   &   & \frac{\partial f_{5}}{\partial z_{1}} &   &   &   &   
\\   &-1 &   &   &   &   &   &   & \frac{\partial f_{9}}{\partial z_{2}} 
\\   &   &-1 &   & \frac{\partial f_{5}}{\partial z_{3}} &   &   &   &   
\\   &   &   &-1 &   &   & \frac{\partial f_{7}}{\partial z_{4}} &   &   
\\   &   &   &   &-1 & \frac{\partial f_{6}}{\partial z_{5}} &   &   &   
\\   &   &   &   &   &-1 & \frac{\partial f_{7}}{\partial z_{6}} &   &   
\\   &   &   &   &   &   &-1 & \frac{\partial f_{8}}{\partial z_{7}} &   
\\   &   &   &   &   &   &   &-1 & \frac{\partial f_{9}}{\partial z_{8}} 
\\   &   &   &   &   &   &   &   & -1 & 
\end{bmatrix}
\begin{bmatrix}
\lambda_{1}\\
\lambda_{2}\\
\lambda_{3}\\
\lambda_{4}\\
\lambda_{5}\\
\lambda_{6}\\
\lambda_{7}\\
\lambda_{8}\\
\lambda_{9}\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
0\\
0\\
0\\
0\\
0\\
0\\
-1
\end{bmatrix}</span> Apply back substitution and we get <script type="math/tex; mode=display"> 
\begin{aligned}
  \lambda_3 &= \lambda_5 \frac{\partial f_5}{\partial z_3}\quad
\lambda_4 = \lambda_7 \frac{\partial f_7}{\partial z_4}\\
\lambda_5 &= \lambda_6 \frac{\partial f_6}{\partial z_6}\quad
\lambda_6 = \lambda_7 \frac{\partial f_7}{\partial z_6}\\
\lambda_7 &= \lambda_8 \frac{\partial f_8}{\partial z_7}\quad
\lambda_8 = \lambda_9 \frac{\partial f_9}{\partial z_8}\\
\lambda_3 &= \frac{\partial l}{\partial z_8} \frac{\partial z_8}{\partial z_7} \frac{\partial z_7}{\partial z_6} \frac{\partial z_6}{\partial z_6} \frac{\partial z_5}{\partial w_1}\\
\lambda_4 &= \frac{\partial l}{\partial z_8} \frac{\partial z_8}{\partial z_7} \frac{\partial z_7}{\partial w_2}
\end{aligned}
</script> 
<span style="display:None">\lambda_3 &= \lambda_5 \frac{\partial f_5}{\partial z_3}\quad
\lambda_4 = \lambda_7 \frac{\partial f_7}{\partial z_4}\\
\lambda_5 &= \lambda_6 \frac{\partial f_6}{\partial z_6}\quad
\lambda_6 = \lambda_7 \frac{\partial f_7}{\partial z_6}\\
\lambda_7 &= \lambda_8 \frac{\partial f_8}{\partial z_7}\quad
\lambda_8 = \lambda_9 \frac{\partial f_9}{\partial z_8}\\
\lambda_3 &= \frac{\partial l}{\partial z_8} \frac{\partial z_8}{\partial z_7} \frac{\partial z_7}{\partial z_6} \frac{\partial z_6}{\partial z_6} \frac{\partial z_5}{\partial w_1}\\
\lambda_4 &= \frac{\partial l}{\partial z_8} \frac{\partial z_8}{\partial z_7} \frac{\partial z_7}{\partial w_2}</span> and there it is!! <script type="math/tex"> \lambda_3 </script> is the gradient for parameter <script type="math/tex"> w_1 </script> and <script type="math/tex"> \lambda_4 </script> represent the gradient of <script type="math/tex"> w_2 </script>.</p><p>Now the structure of matrix <script type="math/tex"> A </script> for this problem isn't that interesting. The example network is very simple. Almost too simple. The computational graph is almost a line graph. But with more interesting cases, like for example, <a href="https://www.cs.unc.edu/ wliu/papers/GoogLeNet.pdf">inception</a> architecture, the matrix will have very nice structure. A very particular example is dense block from <a href="https://arxiv.org/abs/1608.06993">DenseNet</a>. The matrix will have a fully filled upper triangular. </p><p><b>Attribution</b> I had my first encounter with the constrained optimisation view of computation in Yann LeCunn's <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf">1988 paper</a> ``A Theoretical Framework for back propagation''. Incidently, this is the first paper I understood about deep learning and related field. Do give it a read.
