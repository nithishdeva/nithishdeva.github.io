\documentclass[12pt,crop=false,class=article,convert={density=300,outext=.compiled.png}]{standalone}
\usepackage{postmacs}
\usepackage{mathmacs}


\title{\Huge Neural Network Classifier}
\date{}
\author{Nithish Divakar}

\begin{document}
\maketitle

Biggest huddle is keeping track of axes.  Its hard to juggle 3-4 different axes in your head and keep track of which one is to be dotted with which. Then keeping track of them through differentiation is worse. So 

\section*{Everything is a scalar}
\begin{align*}
Z^{(L)}_{ti} &= W^{(L)}_{ij}P^{(L-1)}_{tj}+B^{(L)}_{i}
\\
P^{(L)}_{ti} &= act(Z^{(L)}_{ti})
\end{align*}

It's very easy to implement the forward computation using `einsum` in numpy now. 

\begin{verbatim}
Z2 = np.einsum('ij,tj->ti',W2,P1)+B2
P2 = np.where(Z2>0,Z2,0)    
\end{verbatim}

practically reading of the equation to write the code. 

\section*{Differentiate everything w.r.to everything}
For the first equation in forward computation, we want to find $\frac{\partial Z}{\partial W}$. For this, let differentiate in all the axes

$$\frac{\partial Z_{ab}}{\partial W_{cd}} = P_{ad} ~~when~ b=c$$

Also,

$$\frac{\partial l}{\partial W_{cd}} = \frac{\partial l}{\partial Z_{ab}}\frac{\partial Z_{ab}}{\partial W_{cd}} = [\nabla Z]_{ab}P_{ad}= [\nabla Z]_{ac}P_{ad} $$

\begin{verbatim}
grad_W2 = np.einsum('ac,ad->cd',grad_Z2,P1)
\end{verbatim}

again, implementation is practically reading of the equation. 
\end{document}
