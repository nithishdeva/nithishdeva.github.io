\documentclass[12pt,crop=false,class=article,convert={density=300,outext=.compiled.png}]{standalone}
\usepackage{postmacs}
\usepackage{mathmacs}


\title{\Huge Gradient Through Concatenation}
\date{}
\author{Nithish Divakar}

\begin{document}
\maketitle

Concatenation of vectors is a common operation in computational graph of modern day Deep Learning Networks. How can we compute derivative of the output?
%
$$ z = x\|y$$
%
Where $\|$ is concat operation. We are interested in computing $\sfrac{\partial z}{\partial x}$ and $\sfrac{\partial z}{\partial y} $
%
Assuming $x\in \mathbb{R}^m$ and $x\in \mathbb{R}^n$. We can rewrite the concat operation as
%
$$z = \begin{bmatrix}I_m\\0\end{bmatrix}x+\begin{bmatrix}0\\I_n\end{bmatrix}y$$
%
which gives
%
\begin{align*}
\frac{\partial z}{\partial x} &= \begin{bmatrix}I_m\\0\end{bmatrix}
&
\frac{\partial z}{\partial y} &= \begin{bmatrix}0\\ I_n\end{bmatrix}
\end{align*}

\end{document}
