---
title: Variational Inference
tags : [machine-learning, optimisation, probability]
date: 2023-03-23T08:00:00+05:30
draft: true
---

# Variational Inference

We have some data from a population and we suspect that the it is generated by some underlying process. Estimating the process which generates the data allows us to understand its  fundamental properties. 

Concretely, $p(x)$ is the distribution of the data and $z$ are its latent variables, the process which generates the data is $p(x|z)$. 
Estimating the generation process is computing the true posterior $p(z|x)$. This is the (posterior) inference problem. 

From Bayes rule, we have 
$$p(z|x) = \frac{p(x,z)}{p(x)}$$
with $p(x) = \int_z p(x,z)$. But this is often intractable as the quantities on RHS are non-trivial to estimate. 

Variational inference is the technique which helps in converting this estimation problem into an optimization problem by approximating the posterior $p(z|x)$ with a family of simpler distributions $q_v(z)$. The best approximation is found by minimizing the [divergence] between $q$ and $p$. 

$$ \min_v D_{KL}[q_v(z), p(z|x)] $$

$q_v$ is a family of distribution and a specific member is selected by $v$. In this context, $v$'s are called _variational parameters_.

There are many difference [divergences] measures. But we are going to stick with [KL divergence] as it has a straight forward definition. Note that it is asymmetric.

$$D_{KL}(p,q) = \int_{\Omega} p(x) \log\frac{p(x)}{q(x)}$$

Although this sounds promising and simple in theory, in practise we run into some difficulties. Optimising [KL divergence] is practically difficult. The gradient is readily available for simpler distributions and in practise $p$ can be very complex. This will force us to use very simple $q$ and have a bad approximations.

Let see if we can optimise KL divergence indirectly.

Consider this. $\log p(x)$ is called *evidence* or *log-evidence* of $x$ and is considered a constant. Since $\int_z q_v(z)=1$ we can write

$$\log p(x) = \log p(x) \int_z q_v(z)$$

Since $p(x)$ is invariant to the w.r.to $z$, we have,

$$\log p(x)  = \int_z q_v(z) \log p(x)$$

A bit of clever substitutions and wrangling later,

$$ \log p(x) =\int_z q_v(z) \log \frac{p(x,z)}{q_v(z)} - \int_z q_v(z) \log \frac{p(z|x)}{q_v(z)}$$

Thus we have the following system.

$$\log p(x)    = L(q) - D_{KL}(q,p)$$

$\log p(x)$ is constant and KL divergence is always $\geq 0$. The $L(q)$ is thus lower bound on the approximation between $p$ and $q$. It is hence called *[Evidence Lower Bound]* or ELBO. 

Since a constant is equal to difference between 2 variables, maximising ELBO decreases the KL divergence making approximation of $q$ to $p$ better. Now we have an alternate way to find the best approximation.

---

## Evidence Lower Bound
The evidence lower bound is a combination of 2 entropies. 
$$L(q) = H(q) - H(q,p)$$
While maximizing $L$, the entropy term is pushing $q$ to spread everywhere while the negative cross entropy term is pushing $q$ to concentrate on regions where  $p$ has high density. Thus, maximizing $L$ is a good training objective. 

For this however, we need its gradient. There are 2 main approaches.

We can also write the ELBO in a equivalent form 
$$L(q) = \mathbb{E}_{q_v(x)} \left [ \log p(x,z) - \log q_v(z) \right ]$$


## Score Gradient and Black box VI
Score Gradient. Also called the likelihood ratio

$$\nabla_v L(v) = \mathbb{E}_{q_v(x)} \left [ \nabla_v \log q_v(z)\, (\log p(x,z) - \log q_v(z) )\right ]$$


- Black box VI has 3 criteria. 
1. Sample from $q_v(z)$ 
2.  Evaluate $\nabla_v \log q_v(z)$
3.  Evaluate $\log p(x,z)$ and $\log q_v(z)$
When black box criteria are met, we can optimize $v$ by sampling some data from $q$, evaluate the score gradient and then update $v = v+ \alpha \nabla_v L(v)$. Basic black box inference.

- The gradient has high variance. So it doesn't work that well

## Reprameterisation gradient

$\epsilon \sim s(\epsilon) $
 $   z = t(\epsilon, v)$
    $\implies z \sim q_v(z)$
Then, 
$$\nabla_v L(v) = \mathbb{E}_{s(\epsilon)}\big[  \nabla_z \left[ \log p(x,z) - \log q_v(z) \right]  \; \nabla_v t(\epsilon, v) \big ]$$
The $\nabla_z$ is the definition of the model and can be simply auto-differentiated.
Take samples from $s(\epsilon)$, compute $z = t()$ and then evaluate the reparameterization gradient. This is the idea behind the variational autoencoder. 

stochastic variational inference

This is addressed through black-box variational inference (BBVI), which uses Monte Carlo estimates to replace the manual derivation

When BBVI is combined with SVI by using mini-batches for the gradient estimation, we speak of doubly stochastic estimation.


## Variational Auto-encoders
[Kingma et al. (2013)] Generative models are those which can generate samples from data distribution $ p(x)$. If we model the data distribution as having latent variables, then generating samples similar to $x$ has 2 steps.

- **[encode]** $z\sim  p_\theta(z|x)$ 
- **[decode]** $\hat{x} =  p_\theta(x|z)$

Now, computing the encoder distribution, 
$$ p(z|x) = \frac{ p(x|z) p(z)}{\int_z  p(x|z) p(z)}$$ 
which is intractable. But we can apply variational inference  to approximate the posterior with a simpler distribution $q_{\phi}(z|x)$. Evidence Lower bound of this approximation is


$$\mathcal{E} = \mathbb{E}\_{q_{\phi}(z|x)}\log \frac{ p_\theta(z, x)}{q_{\phi}(z|x) }$$

$$= \mathbb{E}\_{q_{\phi}(z|x)}\log  p_\theta(x|z) +\mathbb{E}\_{q_{\phi}(z)}\log \frac{ p(z)}{q(z|x)}$$

$$= \mathbb{E}\_{q_{phi}(z|x)}\log  p_{\theta}(x|z) -D_{KL}(q_{\phi}(z|x)\| p_\theta(z))$$

Using $q_\phi$ as the encoder network and $ p_\theta$ as the decoder, we have constructed an auto-encoder that can generate samples. This auto-encoder is called Variational Auto-encoder with $\phi$ and $\theta$ as its parameters. 

We can generate samples from a VAE as

- **[compute posterior]** Given a sample $x$, we first compute the posterior distribution
$$x \mapsto q_\phi(z|x)$$
- **[sampling]** From this posterior, we sample a point $z$
$$z\sim q_\phi(z|x)$$
- **[generation]** Now we can generate a sample point by using a completely deterministic network at this point
$$\hat{x} = p_\theta(x|z)$$

Following the variational inference, to train the VAE we have to maximize $\mathcal{E}$.
$$\mathcal{E} =\mathbb{E}\_{q_{\phi}(z|x)}\log  p_\theta(x|z) -D_{KL}(q_{\phi}(z|x)\| p_\theta(z))$$
The first term is maximized when $ p(x|z)$ assigns a high probability to $x$. The second term gives the divergence between $q(z|x)$ and $p(z)$. We can assume a unit normal prior to the latent variables which give  $ p(z)\sim N(0, \mathrm{I})$. We will also assume a gaussian distribution for $q(z|x) \sim \mathcal{N}(\mu,\sigma^2 \mathrm{I})$. 

## References

- [Kingma et al. (2013)]- Kingma, Diederik P., and Max Welling. "**Auto-encoding variational bayes.**" arXiv preprint arXiv:1312.6114 (2013).
- [Blei et al. (2017)]- Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. "**Variational inference: A review for statisticians.**" Journal of the American statistical Association 112.518 (2017): 859-877.

[Kingma et al. (2013)]: <https://arxiv.org/abs/1312.6114> 
   "Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013)."
[Blei et al. (2017)]:   <https://arxiv.org/abs/1601.00670>
   "Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. \"Variational inference: A review for statisticians.\" Journal of the American statistical Association 112.518 (2017): 859-877."

[divergence]:           <https://en.wikipedia.org/wiki/Divergence_(statistics)>
[kl divergence]:        <https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>
[Evidence Lower Bound]: <https://en.wikipedia.org/wiki/Evidence_lower_bound>
