<!doctype html><html style=height:100%><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://daxpy.xyz/favicon.ico><link rel=stylesheet href=/css/style.min.css><link href="//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" rel=stylesheet type=text/css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Variational Inference</title></head><body style=height:100%><div class=header><header id=banner style=display:flex;flex-direction:row;flex-wrap:nowrap;justify-content:flex-start;align-items:baseline><big class=h2 style="flex: 0 0 80px;"><b><a href=https://daxpy.xyz/ class=black>daxpy</a></b></big><nav style=flex:0px><ul style="display:flex;flex-direction:row;gap:1rem;justify-content:safe flex-end"><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>about</a></li></ul></nav></header><hr style=margin:0></div><div class="container justify" style=display:flex;flex-direction:column;min-height:100vh><div class=main style=flex:1><main id=content><section><article><h1 id=variational-inference>Variational Inference</h1><p>We have some data from a population and we suspect that the it is generated by some underlying process. Estimating the process which generates the data allows us to understand its fundamental properties.</p><p>Concretely, $p(x)$ is the distribution of the data and $z$ are its latent variables, the process which generates the data is $p(x|z)$.
Estimating the generation process is computing the true posterior $p(z|x)$. This is the (posterior) inference problem.</p><p>From Bayes rule, we have
$$p(z|x) = \frac{p(x,z)}{p(x)}$$
with $p(x) = \int_z p(x,z)$. But this is often intractable as the quantities on RHS are non-trivial to estimate.</p><p>Variational inference is the technique which helps in converting this estimation problem into an optimization problem by approximating the posterior $p(z|x)$ with a family of simpler distributions $q_v(z)$. The best approximation is found by minimizing the <a href=https://en.wikipedia.org/wiki/Divergence_(statistics) title="Divergence (statistics)">divergence</a> between $q$ and $p$.</p><p>$$ \min_v D_{KL}[q_v(z), p(z|x)] $$</p><p>$q_v$ is a family of distribution and a specific member is selected by $v$. In this context, $v$&rsquo;s are called <em>variational parameters</em>.</p><p>There are many difference [divergences] measures. But we are going to stick with <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence title="Kullback–Leibler divergence">KL divergence</a> as it has a straight forward definition. Note that it is asymmetric.</p><p>$$D_{KL}(p,q) = \int_{\Omega} p \log\frac{p}{q}$$</p><p>Although this sounds promising and simple in theory, in practise we run into some difficulties. Optimising <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence title="Kullback–Leibler divergence">KL divergence</a> is practically difficult. The gradient is readily available for simpler distributions and in practise $p$ can be very complex. This will force us to use very simple $q$ and have a bad approximations.</p><p>Let see if we can optimise KL divergence indirectly.</p><p>Consider this. $\log p(x)$ is called <em>evidence</em> or <em>log-evidence</em> of $x$ and is considered a constant. Since $\int_z q_v(z)=1$ we can write</p><p>$$\log p(x) = \log p(x) \int_z q_v(z)$$</p><p>Since $p(x)$ is invariant to the w.r.to $z$, we have,</p><p>$$\log p(x) = \int_z q_v(z) \log p(x)$$</p><p>A bit of clever substitutions and wrangling later,</p><p>$$ \log p(x) =\int_z q_v(z) \log \frac{p(x,z)}{q_v(z)} - \int_z q_v(z) \log \frac{p(z|x)}{q_v(z)}$$</p><p>Thus we have the following system.</p><p>$$\log p(x) = L(q,p) - D_{KL}(q,p)$$</p><p>$\log p(x)$ is constant and KL divergence is always $\geq 0$. The $L(q,p)$ is thus lower bound on the approximation between $p$ and $q$. It is hence called <em><a href=https://en.wikipedia.org/wiki/Evidence_lower_bound title="Evidence lower bound">Evidence Lower Bound</a></em> or ELBO.</p><p>Since a constant is equal to difference between 2 variables, maximising ELBO decreases the KL divergence making approximation of $q$ to $p$ better. Now we have an alternate way to find the best approximation.</p><h2 id=evidence-lower-bound>Evidence Lower Bound</h2><p>One easy way to understand effect of optimising evidence lower bound is to think of it as a combination of 2 entropies.
$$L(q,p) = H(q) - H(q,p)$$
While maximizing $L$, the entropy term is pushing $q$ to spread everywhere while the negative cross entropy term is pushing $q$ to concentrate on regions where $p$ has high density.
However, for using as a training objective, we need its gradient.</p><h2 id=black-box-variational-inference>Black Box Variational Inference</h2><p>ELBO has the following equivalent form.</p><p>$$L(q,p) = \mathbb{E}_{q_v(z)} \left [ \log p(x,z) - \log q_v(z) \right ]$$</p><p><a href=http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf title='Rajesh Ranganath, Sean Gerrish, and David Blei. "Black box variational inference." Artificial intelligence and statistics. PMLR, 2014.'>Ranganath et al. (2014)</a> gives the following form of gradient.
$$\nabla_v L(v) = \mathbb{E}_{q_v(z)} \left [ \nabla_v \log q_v(z)\, (\log p(x,z) - \log q_v(z) )\right ]$$</p><p>Gradient of the log of probability distribution is called score function and this gradient is known as <strong>score gradient</strong>. When the gradient is in form of an expectation of a random variable, we take monte-carlo samples and to get <a href=https://en.wikipedia.org/wiki/Stochastic_approximation title="Stochastic approximation">approximate gradient</a>.</p><p>Also note that the gradient assumes no knowledge of the model except that we can evaluate the quantities in the equation. More of less, a <em>Black Box Variational Inference</em>.</p><p>Unfortunately, the approximation of the gradient has high variance. Fortunately, <a href=http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf title='Rajesh Ranganath, Sean Gerrish, and David Blei. "Black box variational inference." Artificial intelligence and statistics. PMLR, 2014.'>Ranganath et al. (2014)</a> also describes a few variance reduction techniques.</p><h2 id=reprameterisation-gradient>Reprameterisation gradient</h2><p>Score gradient allows us to use complex distributions to approximate posterior distribution. But it is difficult to sample from complex distributions. <a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Reparametrisation trick</a> allows us to create complex distributions from simple ones.</p><p>Say we are able to write $z = t(\epsilon, v)$ with
$\epsilon \sim s(\epsilon)$, a simple distribution which we can sample from.
What we have done is to bound all &ldquo;randomness&rdquo; to $s(\epsilon)$ and made $q_v(z)$ &ldquo;non-random&rdquo;.</p><p>With this, we have a simpler gradient for elbo.</p><p>$$\nabla_v L(v) = \mathbb{E}_{s(\epsilon)}\big[ \nabla_z \left[ \log p(x,z) - \log q_v(z) \right] \nabla_v t(\epsilon, v) \big]$$</p><p>To compute approximate gradient, take samples $\epsilon \sim s(\epsilon)$, compute $z = t(\epsilon, v)$ and then evaluate the reparameterization gradient.
$\log p(x,z) - \log q_v(z)$ is the model and $\nabla_z(\cdots)$ can be evaluated using auto-differentiation. See <a href=http://www.cs.columbia.edu/~blei/papers/RuizTitsiasBlei2016b.pdf title='Ruiz, Francisco R., Titsias RC AUEB, and David Blei. "The generalized reparameterization gradient." Advances in neural information processing systems 29 (2016).'>Ruiz et al. (2016)</a> for more discussion on reprameterisation gradient.</p><h2 id=amortised-variational-inference>Amortised Variational Inference</h2><p>Variational Inference is still not scalable. We still have to fit the variational parameter for each observations essentially minimising KL divergence between each $q_v(z)$ and $p(z|x_i)$. This is not scalable and will lead to over fitting.</p><p>We can get around this by constrianing the variational parameters to be a function of the observations; a learnable function.</p><p>$$v = g_\phi(x)$$</p><p>In <a href=https://www.tuananhle.co.uk/notes/amortized-inference.html title="Amortized Inference">Amortised Inference</a>, the variational parameters are output of a network which takes the samples from true distribution as input. The parameters of the network can be learned by optimising reparameterisation gradient.</p><p>$$\nabla_\phi L(\phi) = \mathbb{E}_{s(\epsilon)}\big[ \nabla_z \left[ \log p(x,z) - \log q_v(z) \right] \nabla_\phi t(\epsilon, g_\phi(x)) \big]$$</p><h2 id=variational-auto-encoders>Variational Auto Encoders</h2><p><a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Variational auto encoders</a> are a generative model which learns to generate data from its true distribution. It has an architecture simlar to a <a href=https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf title='Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion". Journal of machine learning research, 11(12).'>denoising auto encoder</a> and uses variational inference to learn the distribution.</p><p>Variational auto encoders make a constraint that the posterior is approximated by an gaussian distribution with diagonal covariances. As a result, the latent representation will have linearly independent dimensions.</p><p>The encoder is a differentiable network which is used to approximate posterior distribution $p(z|x)$. The network is trained to predict the parameters of the approximating distribution from a data point.</p><p>$$\mu, \sigma = g_{\phi}(x)$$</p><p>then, the posterior is approximated by the distribution
$$ q_{\mu, \sigma}(z)= \mathcal{N}(z; \mu, diag(\sigma^2))$$</p><p>The latent vector is obtained by sampling from $q_{\mu, \sigma}(z)$.</p><p>The decoder is also a differentiable network which is trained to predict a sample from the latent vector.
$$\hat{x} = f_{\theta}(z)$$</p><p>Different samples generate different predictions thus generating samples from $p(x|z)$. Some designs explicitly adds &ldquo;intelligent noise&rdquo; to aid in directed distribution. $\hat{x} = f_{\theta}(z+\epsilon)$</p><p>The forward inference of variational auto encoder defined in the <a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Kingma et al. (2013)</a> has the following form.</p><p>$$(\mu, \log \sigma) = g_\phi(x) $$
$$q_{\mu,\sigma}(z|x) = \mathcal{N} (z; \mu, diag(\sigma^2))$$
$$z \sim q_{\mu,\sigma}(z|x)$$
$$ \hat{x} = f_{\theta}(z)$$</p><p>Both the encoder and decoder are neural networks and $\phi$ and $\theta$ are their parameters. Both the networks are trained end to end to miniminse the following loss.
$$L
= C\|x - f(z) \|^2 + D_{KL}(\mathcal{N}(\mu, diag(\sigma^2), \mathcal{N}(0,I))$$</p><h2 id=references>References</h2><ul><li><p>Kingma, Diederik P., and Max Welling. &ldquo;<a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>Auto-encoding variational bayes.</a>&rdquo; arXiv preprint arXiv:1312.6114 (2013).</p></li><li><p>Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. &ldquo;<a href=https://arxiv.org/abs/1601.00670 title='Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. "Variational inference: A review for statisticians." Journal of the American statistical Association 112.518 (2017): 859-877.'>Variational inference: A review for statisticians.</a>&rdquo; Journal of the American statistical Association 112.518 (2017): 859-877.</p></li><li><p>Rajesh Ranganath, Sean Gerrish, and David Blei. &ldquo;<a href=http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf title='Rajesh Ranganath, Sean Gerrish, and David Blei. "Black box variational inference." Artificial intelligence and statistics. PMLR, 2014.'>Black box variational inference.</a>&rdquo; Artificial intelligence and statistics. PMLR, 2014.</p></li><li><p>Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). &ldquo;<a href=https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf title='Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A., & Bottou, L. (2010). "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion". Journal of machine learning research, 11(12).'>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</a>&rdquo;. Journal of machine learning research, 11(12).</p></li><li><p>Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. &ldquo;<a href=https://arxiv.org/pdf/1401.4082.pdf title='Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. "Stochastic backpropagation and approximate inference in deep generative models." International conference on machine learning. PMLR, 2014.'>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</a>.&rdquo; International conference on machine learning. PMLR, 2014.</p></li><li><p>Diederik P. Kingma and Max Welling (2019), &ldquo;<a href=https://arxiv.org/abs/1312.6114 title='Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).'>An Introduction to Variational Autoencoders</a>&rdquo;, Foundations and Trends in Machine Learning: Vol. xx, No.xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.</p></li><li><p>&ldquo;Variational inference&rdquo; <a href=https://ermongroup.github.io/cs228-notes/inference/variational>https://ermongroup.github.io/cs228-notes/inference/variational</a></p></li><li><p>&ldquo;The variational auto-encoder&rdquo; <a href=https://ermongroup.github.io/cs228-notes/extras/vae>https://ermongroup.github.io/cs228-notes/extras/vae</a></p></li><li><p>&ldquo;Variational Inference with Normalizing Flows&rdquo; <a href=https://www.depthfirstlearning.com/2021/VI-with-NFs>https://www.depthfirstlearning.com/2021/VI-with-NFs</a></p></li><li><p>&ldquo;Amortized Inference and Variational Auto Encoders&rdquo; <a href=https://erdogdu.github.io/csc412/notes/lec11-1.pdf>https://erdogdu.github.io/csc412/notes/lec11-1.pdf</a></p></li><li><p>&ldquo;Amortized Inference&rdquo; <a href=https://www.tuananhle.co.uk/notes/amortized-inference.html>https://www.tuananhle.co.uk/notes/amortized-inference.html</a></p></li><li><p>&ldquo;Variational Inference: Foundations and Innovations&rdquo; <a href="https://www.youtube.com/watch?v=Dv86zdWjJKQ">https://www.youtube.com/watch?v=Dv86zdWjJKQ</a></p></li></ul></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a>
<a class=gray style=padding-right:5px href=/tags#optimisation><i>#optimisation</i></a>
<a class=gray style=padding-right:5px href=/tags#probability><i>#probability</i></a></section></main></div><div class=footer><br><br><hr style=margin:0><footer id=footer style=display:flex;justify-content:space-between><div>Ping me <a target=_blank class=green href=https://twitter.com/nithishdivakar>@nithishdivakar</a></div><div><a href=/resources target=_blank>::</a></div></footer></div></div></body></html>