<!doctype html><html style=height:100%><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://daxpy.xyz/favicon.ico><link rel=stylesheet href=/css/style.min.css><link href="//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" rel=stylesheet type=text/css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Focal Loss</title></head><body style=height:100%><div class=container style=display:flex;flex-direction:column;min-height:100vh><div class=header><header id=banner style=display:flex;flex-direction:row;flex-wrap:nowrap;justify-content:flex-start;align-items:baseline><big class=h2 style="flex: 0 0 80px;"><b><a href=https://daxpy.xyz/ class=black>daxpy</a></b></big><nav style=flex:0px><ul style=display:flex;flex-direction:row;gap:.5rem><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>about</a></li></ul></nav></header><hr style=margin:0></div><div class=main style=flex:1><main id=content><section><h1 id=focal-loss>Focal Loss</h1><p>For binary classification problem, the standard cross entropy loss is given by</p><p>$$CE(p,y_t)=\begin{cases}-\log(p)&y_t=1
\\
-\log(1-p)&else\end{cases}$$</p><p>We can simplify this to $CE(p_t) = -\log(p_t)$ if we define
$$p_t \mathop{\mathrm{\triangleq}}
\begin{cases}p&y_t=1
\\
1-p&else\end{cases}$$</p><p>What if there is a huge imbalance between no of positive and negative samples? The standard way of fixing this would be to add a balancing term $\alpha$ which is derived from inverse class frequencies. Let</p><p>$$\alpha_t \mathop{\mathrm{\triangleq}}\begin{cases}\alpha&y_t=1\\1-\alpha&else\end{cases}$$</p><p>Balanced cross entropy loss is then, $$CE(p_t) = -\alpha_t\log(p_t)$$</p><p>However, the class imbalance has another effect during training which cannot be mitigated by this balancing factor. The model will learn to predict the larger class quickly than the smaller one because it simply has more samples. Focal loss was introduced to fix this problem.</p><p>If we look at the values of $p_t$, we can see clear distinction. The samples where $p_t \to 1$ are the ones where the model is confident about or easy examples. $p_t&lt;0.5$ however are hard examples.</p><p>We would want the model to care most about the hard samples. So we can add a modulating function, $(1-p_t)^\gamma$ to the cross entropy loss.
$\gamma$ is a hyper parameter which controls the severity of the modulating function.</p><p>So we define focal loss as $$FL(p_t)=-(1-p_t)^\gamma \log(p_t)$$ From the plots we can see that the loss value are highly diminished for easy examples. Also, if we compare the ratio of Cross entropy loss to Focal Loss given by $\frac{1}{(1-p_t)^\gamma}$, the value is huge as $p_t \to 1$.</p><p>For a dataset with class imbalance, we can add a class weight term to the loss.</p><p>$$FL(p_t) = -\alpha_t(1-p_t)^\gamma \log(p_t)$$</p><p>Focal loss can be extended to to multi class setting as
$$FL(y,p) = -\sum_{c\in \mathop{\mathrm{\mathcal{C}}}} y_c\alpha(1-p_c)^\gamma \log(p_c)$$</p><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning>machine-learning</a>
<a class=gray style=padding-right:5px href=/tags#deep-learning>deep-learning</a></section></main></div><div class=footer><br><br><hr style=margin:0><footer id=footer style=display:flex;justify-content:space-between><div>Ping me <a target=_blank class=green href=https://twitter.com/nithishdivakar>@nithishdivakar</a></div><div><a href=/resources target=_blank>::</a></div></footer></div></div></body></html>