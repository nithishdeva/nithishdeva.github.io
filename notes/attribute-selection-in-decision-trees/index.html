<!doctype html><html style=height:100%><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://daxpy.xyz/favicon.ico><link rel=stylesheet href=/css/style.min.css><link href="//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" rel=stylesheet type=text/css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><title>Attribute Selection in Decision Trees</title></head><body style=height:100%><div class=header><header id=banner style=display:flex;flex-direction:row;flex-wrap:nowrap;justify-content:flex-start;align-items:baseline><big class=h2 style="flex:0 0 80px"><b><a href=https://daxpy.xyz/ class=black>daxpy</a></b></big><nav style=flex:0px><ul style="display:flex;flex-direction:row;gap:1rem;justify-content:safe flex-end"><li><a href=https://daxpy.xyz/notes/>Notes</a></li><li><a href=https://daxpy.xyz/posts/>Posts</a></li><li><a href=https://daxpy.xyz/links/>Links</a></li><li><a href=https://daxpy.xyz/links_fav/>Links</a></li><li><a href=https://daxpy.xyz/stories/>Stories</a></li><li><a href=/about>about</a></li></ul></nav></header><hr style=margin:0></div><div class="container justify" style=display:flex;flex-direction:column;min-height:100vh><div class=main style=flex:1><main id=content><section><article><h1 id=attribute-selection-in-decision-trees>Attribute Selection in Decision Trees</h1><p>For constructing a new node in decision tree, choosing which attribute to partition the data on is important. Choosing a less desirable attribute to split the data on may result in lower performance. Lets look into a few important measures which helps us find the best attribute.</p><h2 id=information-gain>Information Gain</h2><p>Information Gain is defined as amount of information gained about a random variable (outcome) from observing another (attribute).
We can quantify information gain as difference in entropy when random variable is observed.</p><p>$$\begin{aligned}
IG(T,A) &= H(T) - H(T|A)
\\
H(T) &= -\sum_{c\in C}^{}p_c\log_2 p_c
\\
H(T|A)
&= \sum_{a
\in A}p_a H(T_a)
\end{aligned}$$
Here $H(T)$ is the entropy of set $T$ and $T_a = \{t\in T: t_{A} = a\}$ is its subset of items with attribute $A=a$. Also, $p_a = \frac{\left|T_a\right|}{|T|}$.</p><h2 id=gini-impurity>GINI Impurity</h2><p>GINI Impurity is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it was randomly labeled according to the distribution of an attribute in the set.</p><p>Let say we partition the input set $T$ according to the values of attribute $A$ such that $T = \bigcup_{a\in A} T_a$.
The split would be ideal if each of the partitions would have only a single class (different subset can have same class).</p><p>GINI impurity quantifies having multiple classes in same partition.
$$\begin{aligned}
G(T_a) &=
\sum_{c\in C}p_{a,c}\left(\textstyle\sum_{k \neq c} p_{a,k}\right)
\\
&=\sum_{c\in C}p_{a,c}(1-p_{a,c})
\\
&= 1 - \sum_{c\in C}p_{a,c}^2
\end{aligned}$$</p><p>Overall GINI Impurity score of partitioning $T$ according to $A$ is
$$\begin{aligned}
G(A) &=
\sum_{a\in A}p_{a}G(T_a) \quad
\\
&= \sum_{a\in A} p_{a} \left(1- \sum_{c\in C}p_{a,c}^{2}\right)
\end{aligned}$$</p><p>$p_a$ fraction of elements which has attribute $a$
$p_{a,c}$ fraction of elements in class $c$ and has attribute $a$</p><h2 id=variance-reduction>Variance Reduction</h2><p>Variance reduction is used when target variable is continuous (tree is a regression tree). If the set $T$ is being partitioned into $T_L$ and $T_R$, the reduction in variance is given by
$$\begin{aligned}
V(T) = Var(T) &- \left(\frac{|T_L|}{|T|}Var(T_L)+\frac{|T_R|}{|T|}Var(T_R)\right)<br>\end{aligned}$$
For calculating the best split point, the standard variance calculation formula would require recalculation of mean repeatedly. But we can compute variance without explicitly calculating mean as
$$Var(S) = \frac{1}{|S|^2}\sum_{i,j\in S}(y_i-y_j)^2 $$</p></article><br><br><br><i>tags:</i>
<a class=gray style=padding-right:5px href=/tags#machine-learning><i>#machine-learning</i></a></section></main></div><div class=footer><br><br><hr style=margin:0><footer id=footer style=display:flex;justify-content:space-between><div>Ping me <a target=_blank class=green href=https://twitter.com/nithishdivakar>@nithishdivakar</a></div><div><a href=/resources target=_blank>::</a></div></footer></div></div></body></html>